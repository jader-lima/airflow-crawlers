[2023-01-04 08:57:18,314] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_job_dag.spark_ingestion scheduled__2022-12-29T09:00:00+00:00 [queued]>
[2023-01-04 08:57:18,325] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_job_dag.spark_ingestion scheduled__2022-12-29T09:00:00+00:00 [queued]>
[2023-01-04 08:57:18,325] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-01-04 08:57:18,325] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-01-04 08:57:18,325] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-01-04 08:57:18,374] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): spark_ingestion> on 2022-12-29 09:00:00+00:00
[2023-01-04 08:57:18,379] {standard_task_runner.py:52} INFO - Started process 418 to run task
[2023-01-04 08:57:18,382] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'spark_job_dag', 'spark_ingestion', 'scheduled__2022-12-29T09:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/spark_job_dag.py', '--cfg-path', '/tmp/tmpm3f6fwrs', '--error-file', '/tmp/tmp29w4fyqu']
[2023-01-04 08:57:18,382] {standard_task_runner.py:80} INFO - Job 3: Subtask spark_ingestion
[2023-01-04 08:57:18,541] {task_command.py:369} INFO - Running <TaskInstance: spark_job_dag.spark_ingestion scheduled__2022-12-29T09:00:00+00:00 [running]> on host dceedb0d05e0
[2023-01-04 08:57:18,875] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=spark_job_dag
AIRFLOW_CTX_TASK_ID=spark_ingestion
AIRFLOW_CTX_EXECUTION_DATE=2022-12-29T09:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-29T09:00:00+00:00
[2023-01-04 08:57:18,886] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-04 08:57:18,887] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://***-crawlers-spark-master-1:7077 --name spark_ingestion --verbose /opt/spark/ingestion/ingestion.py --src /opt/spark-data/datalake/transient/teste --dest /opt/spark-data/datalake/bronze/teste/extract_date=2022-12-29 --table_name teste --src_format csv --dest_format parquet --process_date 2022-12-29
[2023-01-04 08:57:18,974] {spark_submit.py:495} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-04 08:57:20,960] {spark_submit.py:495} INFO - Using properties file: null
[2023-01-04 08:57:21,095] {spark_submit.py:495} INFO - Parsed arguments:
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - master                  spark://***-crawlers-spark-master-1:7077
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - deployMode              null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - executorMemory          null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - executorCores           null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - totalExecutorCores      null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - propertiesFile          null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - driverMemory            null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - driverCores             null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - driverExtraClassPath    null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - driverExtraLibraryPath  null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - driverExtraJavaOptions  null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - supervise               false
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - queue                   null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - numExecutors            null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - files                   null
[2023-01-04 08:57:21,096] {spark_submit.py:495} INFO - pyFiles                 null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - archives                null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - mainClass               null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - primaryResource         file:/opt/spark/ingestion/ingestion.py
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - name                    spark_ingestion
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - childArgs               [--src /opt/spark-data/datalake/transient/teste --dest /opt/spark-data/datalake/bronze/teste/extract_date=2022-12-29 --table_name teste --src_format csv --dest_format parquet --process_date 2022-12-29]
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - jars                    null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - packages                null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - packagesExclusions      null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - repositories            null
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - verbose                 true
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - Spark properties used, including those specified through
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - --conf and those from the properties file null:
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:21,097] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:21,395] {spark_submit.py:495} INFO - Main class:
[2023-01-04 08:57:21,395] {spark_submit.py:495} INFO - org.apache.spark.deploy.PythonRunner
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - Arguments:
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - file:/opt/spark/ingestion/ingestion.py
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - null
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - --src
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - /opt/spark-data/datalake/transient/teste
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - --dest
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - /opt/spark-data/datalake/bronze/teste/extract_date=2022-12-29
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - --table_name
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - teste
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - --src_format
[2023-01-04 08:57:21,396] {spark_submit.py:495} INFO - csv
[2023-01-04 08:57:21,397] {spark_submit.py:495} INFO - --dest_format
[2023-01-04 08:57:21,397] {spark_submit.py:495} INFO - parquet
[2023-01-04 08:57:21,397] {spark_submit.py:495} INFO - --process_date
[2023-01-04 08:57:21,397] {spark_submit.py:495} INFO - 2022-12-29
[2023-01-04 08:57:21,400] {spark_submit.py:495} INFO - Spark config:
[2023-01-04 08:57:21,400] {spark_submit.py:495} INFO - (spark.app.name,spark_ingestion)
[2023-01-04 08:57:21,400] {spark_submit.py:495} INFO - (spark.app.submitTime,1672833441373)
[2023-01-04 08:57:21,400] {spark_submit.py:495} INFO - (spark.master,spark://***-crawlers-spark-master-1:7077)
[2023-01-04 08:57:21,400] {spark_submit.py:495} INFO - (spark.submit.deployMode,client)
[2023-01-04 08:57:21,401] {spark_submit.py:495} INFO - (spark.submit.pyFiles,)
[2023-01-04 08:57:21,401] {spark_submit.py:495} INFO - Classpath elements:
[2023-01-04 08:57:21,401] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:21,401] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:21,401] {spark_submit.py:495} INFO - 
[2023-01-04 08:57:22,066] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SparkContext: Running Spark version 3.3.1
[2023-01-04 08:57:22,129] {spark_submit.py:495} INFO - 23/01/04 08:57:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-04 08:57:22,223] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO ResourceUtils: ==============================================================
[2023-01-04 08:57:22,223] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-04 08:57:22,224] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO ResourceUtils: ==============================================================
[2023-01-04 08:57:22,224] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SparkContext: Submitted application: ingestion
[2023-01-04 08:57:22,242] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-04 08:57:22,255] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO ResourceProfile: Limiting resource is cpu
[2023-01-04 08:57:22,256] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-04 08:57:22,297] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SecurityManager: Changing view acls to: root
[2023-01-04 08:57:22,298] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SecurityManager: Changing modify acls to: root
[2023-01-04 08:57:22,298] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SecurityManager: Changing view acls groups to:
[2023-01-04 08:57:22,299] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SecurityManager: Changing modify acls groups to:
[2023-01-04 08:57:22,299] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[2023-01-04 08:57:22,598] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO Utils: Successfully started service 'sparkDriver' on port 44317.
[2023-01-04 08:57:22,622] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SparkEnv: Registering MapOutputTracker
[2023-01-04 08:57:22,655] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-04 08:57:22,674] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-04 08:57:22,675] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-04 08:57:22,678] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-04 08:57:22,698] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07da9e16-3561-456a-8a39-67ec206f3095
[2023-01-04 08:57:22,715] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-04 08:57:22,733] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-04 08:57:22,947] {spark_submit.py:495} INFO - 23/01/04 08:57:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-04 08:57:23,065] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://***-crawlers-spark-master-1:7077...
[2023-01-04 08:57:23,102] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO TransportClientFactory: Successfully created connection to ***-crawlers-spark-master-1/172.21.0.3:7077 after 22 ms (0 ms spent in bootstraps)
[2023-01-04 08:57:23,216] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230104115723-0000
[2023-01-04 08:57:23,225] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38423.
[2023-01-04 08:57:23,225] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO NettyBlockTransferService: Server created on dceedb0d05e0:38423
[2023-01-04 08:57:23,227] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-04 08:57:23,235] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dceedb0d05e0, 38423, None)
[2023-01-04 08:57:23,239] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO BlockManagerMasterEndpoint: Registering block manager dceedb0d05e0:38423 with 434.4 MiB RAM, BlockManagerId(driver, dceedb0d05e0, 38423, None)
[2023-01-04 08:57:23,242] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dceedb0d05e0, 38423, None)
[2023-01-04 08:57:23,245] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dceedb0d05e0, 38423, None)
[2023-01-04 08:57:23,246] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/0 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 08:57:23,248] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/0 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 08:57:23,463] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/0 is now RUNNING
[2023-01-04 08:57:23,616] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2023-01-04 08:57:23,819] {spark_submit.py:495} INFO - /opt/spark-data/datalake/transient/teste
[2023-01-04 08:57:23,819] {spark_submit.py:495} INFO - /opt/spark-data/datalake/bronze/teste/extract_date=2022-12-29
[2023-01-04 08:57:23,819] {spark_submit.py:495} INFO - teste
[2023-01-04 08:57:23,820] {spark_submit.py:495} INFO - csv
[2023-01-04 08:57:23,820] {spark_submit.py:495} INFO - parquet
[2023-01-04 08:57:23,820] {spark_submit.py:495} INFO - 2022-12-29
[2023-01-04 08:57:23,904] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-04 08:57:23,914] {spark_submit.py:495} INFO - 23/01/04 08:57:23 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-04 08:57:25,279] {spark_submit.py:495} INFO - 23/01/04 08:57:25 INFO InMemoryFileIndex: It took 95 ms to list leaf files for 1 paths.
[2023-01-04 08:57:25,367] {spark_submit.py:495} INFO - 23/01/04 08:57:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-01-04 08:57:27,358] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-04 08:57:27,359] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-04 08:57:27,361] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-01-04 08:57:27,612] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.6 KiB, free 434.2 MiB)
[2023-01-04 08:57:27,651] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
[2023-01-04 08:57:27,654] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dceedb0d05e0:38423 (size: 34.0 KiB, free: 434.4 MiB)
[2023-01-04 08:57:27,657] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-04 08:57:27,663] {spark_submit.py:495} INFO - 23/01/04 08:57:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-04 08:57:28,179] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO CodeGenerator: Code generated in 123.670713 ms
[2023-01-04 08:57:28,210] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-04 08:57:28,225] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-04 08:57:28,226] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-04 08:57:28,226] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-04 08:57:28,228] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO DAGScheduler: Missing parents: List()
[2023-01-04 08:57:28,232] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-04 08:57:28,309] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.2 KiB, free 434.2 MiB)
[2023-01-04 08:57:28,311] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-04 08:57:28,312] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on dceedb0d05e0:38423 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-04 08:57:28,313] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-04 08:57:28,325] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-04 08:57:28,326] {spark_submit.py:495} INFO - 23/01/04 08:57:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-04 08:57:43,344] {spark_submit.py:495} INFO - 23/01/04 08:57:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:57:58,343] {spark_submit.py:495} INFO - 23/01/04 08:57:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:58:13,343] {spark_submit.py:495} INFO - 23/01/04 08:58:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:58:28,343] {spark_submit.py:495} INFO - 23/01/04 08:58:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:58:43,343] {spark_submit.py:495} INFO - 23/01/04 08:58:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:58:58,343] {spark_submit.py:495} INFO - 23/01/04 08:58:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:59:13,343] {spark_submit.py:495} INFO - 23/01/04 08:59:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:59:26,016] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/0 is now EXITED (Command exited with code 1)
[2023-01-04 08:59:26,018] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/0 removed: Command exited with code 1
[2023-01-04 08:59:26,020] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/1 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 08:59:26,020] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/1 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 08:59:26,022] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO BlockManagerMaster: Removal of executor 0 requested
[2023-01-04 08:59:26,024] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
[2023-01-04 08:59:26,026] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
[2023-01-04 08:59:26,057] {spark_submit.py:495} INFO - 23/01/04 08:59:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/1 is now RUNNING
[2023-01-04 08:59:28,343] {spark_submit.py:495} INFO - 23/01/04 08:59:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:59:43,343] {spark_submit.py:495} INFO - 23/01/04 08:59:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 08:59:58,343] {spark_submit.py:495} INFO - 23/01/04 08:59:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:00:13,343] {spark_submit.py:495} INFO - 23/01/04 09:00:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:00:28,343] {spark_submit.py:495} INFO - 23/01/04 09:00:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:00:43,342] {spark_submit.py:495} INFO - 23/01/04 09:00:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:00:58,342] {spark_submit.py:495} INFO - 23/01/04 09:00:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:01:13,343] {spark_submit.py:495} INFO - 23/01/04 09:01:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:01:27,746] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/1 is now EXITED (Command exited with code 1)
[2023-01-04 09:01:27,746] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/1 removed: Command exited with code 1
[2023-01-04 09:01:27,747] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/2 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:01:27,747] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/2 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:01:27,748] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO BlockManagerMaster: Removal of executor 1 requested
[2023-01-04 09:01:27,748] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 1
[2023-01-04 09:01:27,748] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
[2023-01-04 09:01:27,783] {spark_submit.py:495} INFO - 23/01/04 09:01:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/2 is now RUNNING
[2023-01-04 09:01:28,343] {spark_submit.py:495} INFO - 23/01/04 09:01:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:01:43,342] {spark_submit.py:495} INFO - 23/01/04 09:01:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:01:58,343] {spark_submit.py:495} INFO - 23/01/04 09:01:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:02:13,343] {spark_submit.py:495} INFO - 23/01/04 09:02:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:02:28,343] {spark_submit.py:495} INFO - 23/01/04 09:02:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:02:43,343] {spark_submit.py:495} INFO - 23/01/04 09:02:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:02:58,343] {spark_submit.py:495} INFO - 23/01/04 09:02:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:03:13,344] {spark_submit.py:495} INFO - 23/01/04 09:03:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:03:28,343] {spark_submit.py:495} INFO - 23/01/04 09:03:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:03:29,951] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/2 is now EXITED (Command exited with code 1)
[2023-01-04 09:03:29,951] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/2 removed: Command exited with code 1
[2023-01-04 09:03:29,952] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
[2023-01-04 09:03:29,952] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO BlockManagerMaster: Removal of executor 2 requested
[2023-01-04 09:03:29,952] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/3 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:03:29,952] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 2
[2023-01-04 09:03:29,952] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/3 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:03:29,984] {spark_submit.py:495} INFO - 23/01/04 09:03:29 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/3 is now RUNNING
[2023-01-04 09:03:43,343] {spark_submit.py:495} INFO - 23/01/04 09:03:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:03:58,343] {spark_submit.py:495} INFO - 23/01/04 09:03:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:04:13,343] {spark_submit.py:495} INFO - 23/01/04 09:04:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:04:28,343] {spark_submit.py:495} INFO - 23/01/04 09:04:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:04:43,343] {spark_submit.py:495} INFO - 23/01/04 09:04:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:04:58,343] {spark_submit.py:495} INFO - 23/01/04 09:04:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:05:13,343] {spark_submit.py:495} INFO - 23/01/04 09:05:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:05:28,343] {spark_submit.py:495} INFO - 23/01/04 09:05:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:05:31,561] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/3 is now EXITED (Command exited with code 1)
[2023-01-04 09:05:31,561] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/3 removed: Command exited with code 1
[2023-01-04 09:05:31,562] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/4 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:05:31,562] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/4 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:05:31,563] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
[2023-01-04 09:05:31,563] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO BlockManagerMaster: Removal of executor 3 requested
[2023-01-04 09:05:31,563] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 3
[2023-01-04 09:05:31,593] {spark_submit.py:495} INFO - 23/01/04 09:05:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/4 is now RUNNING
[2023-01-04 09:05:43,343] {spark_submit.py:495} INFO - 23/01/04 09:05:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:05:58,343] {spark_submit.py:495} INFO - 23/01/04 09:05:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:06:13,343] {spark_submit.py:495} INFO - 23/01/04 09:06:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:06:28,343] {spark_submit.py:495} INFO - 23/01/04 09:06:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:06:43,343] {spark_submit.py:495} INFO - 23/01/04 09:06:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:06:58,343] {spark_submit.py:495} INFO - 23/01/04 09:06:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:07:13,343] {spark_submit.py:495} INFO - 23/01/04 09:07:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:07:28,343] {spark_submit.py:495} INFO - 23/01/04 09:07:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:07:33,175] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/4 is now EXITED (Command exited with code 1)
[2023-01-04 09:07:33,175] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/4 removed: Command exited with code 1
[2023-01-04 09:07:33,175] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
[2023-01-04 09:07:33,176] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/5 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:07:33,176] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO BlockManagerMaster: Removal of executor 4 requested
[2023-01-04 09:07:33,176] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 4
[2023-01-04 09:07:33,176] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/5 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:07:33,202] {spark_submit.py:495} INFO - 23/01/04 09:07:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/5 is now RUNNING
[2023-01-04 09:07:43,343] {spark_submit.py:495} INFO - 23/01/04 09:07:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:07:58,343] {spark_submit.py:495} INFO - 23/01/04 09:07:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:08:13,342] {spark_submit.py:495} INFO - 23/01/04 09:08:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:08:28,343] {spark_submit.py:495} INFO - 23/01/04 09:08:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:08:43,342] {spark_submit.py:495} INFO - 23/01/04 09:08:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:08:58,343] {spark_submit.py:495} INFO - 23/01/04 09:08:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:09:13,342] {spark_submit.py:495} INFO - 23/01/04 09:09:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:09:28,343] {spark_submit.py:495} INFO - 23/01/04 09:09:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:09:34,783] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/5 is now EXITED (Command exited with code 1)
[2023-01-04 09:09:34,783] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/5 removed: Command exited with code 1
[2023-01-04 09:09:34,783] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
[2023-01-04 09:09:34,783] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/6 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:09:34,783] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO BlockManagerMaster: Removal of executor 5 requested
[2023-01-04 09:09:34,783] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 5
[2023-01-04 09:09:34,784] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/6 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:09:34,807] {spark_submit.py:495} INFO - 23/01/04 09:09:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/6 is now RUNNING
[2023-01-04 09:09:43,342] {spark_submit.py:495} INFO - 23/01/04 09:09:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:09:58,343] {spark_submit.py:495} INFO - 23/01/04 09:09:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:10:13,343] {spark_submit.py:495} INFO - 23/01/04 09:10:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:10:28,342] {spark_submit.py:495} INFO - 23/01/04 09:10:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:10:43,343] {spark_submit.py:495} INFO - 23/01/04 09:10:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:10:58,343] {spark_submit.py:495} INFO - 23/01/04 09:10:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:11:13,343] {spark_submit.py:495} INFO - 23/01/04 09:11:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:11:28,342] {spark_submit.py:495} INFO - 23/01/04 09:11:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:11:36,421] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/6 is now EXITED (Command exited with code 1)
[2023-01-04 09:11:36,421] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/6 removed: Command exited with code 1
[2023-01-04 09:11:36,421] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
[2023-01-04 09:11:36,421] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO BlockManagerMaster: Removal of executor 6 requested
[2023-01-04 09:11:36,422] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/7 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:11:36,422] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 6
[2023-01-04 09:11:36,422] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/7 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:11:36,449] {spark_submit.py:495} INFO - 23/01/04 09:11:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/7 is now RUNNING
[2023-01-04 09:11:43,343] {spark_submit.py:495} INFO - 23/01/04 09:11:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:11:58,343] {spark_submit.py:495} INFO - 23/01/04 09:11:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:12:13,343] {spark_submit.py:495} INFO - 23/01/04 09:12:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:12:28,342] {spark_submit.py:495} INFO - 23/01/04 09:12:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:12:43,343] {spark_submit.py:495} INFO - 23/01/04 09:12:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:12:58,343] {spark_submit.py:495} INFO - 23/01/04 09:12:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:13:13,343] {spark_submit.py:495} INFO - 23/01/04 09:13:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:13:28,342] {spark_submit.py:495} INFO - 23/01/04 09:13:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:13:38,014] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/7 is now EXITED (Command exited with code 1)
[2023-01-04 09:13:38,014] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO StandaloneSchedulerBackend: Executor app-20230104115723-0000/7 removed: Command exited with code 1
[2023-01-04 09:13:38,014] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230104115723-0000/8 on worker-20230104114732-172.21.0.6-7000 (172.21.0.6:7000) with 1 core(s)
[2023-01-04 09:13:38,015] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO BlockManagerMasterEndpoint: Trying to remove executor 7 from BlockManagerMaster.
[2023-01-04 09:13:38,015] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO BlockManagerMaster: Removal of executor 7 requested
[2023-01-04 09:13:38,015] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 7
[2023-01-04 09:13:38,015] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO StandaloneSchedulerBackend: Granted executor ID app-20230104115723-0000/8 on hostPort 172.21.0.6:7000 with 1 core(s), 1024.0 MiB RAM
[2023-01-04 09:13:38,041] {spark_submit.py:495} INFO - 23/01/04 09:13:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230104115723-0000/8 is now RUNNING
[2023-01-04 09:13:43,343] {spark_submit.py:495} INFO - 23/01/04 09:13:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:13:58,343] {spark_submit.py:495} INFO - 23/01/04 09:13:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:14:13,342] {spark_submit.py:495} INFO - 23/01/04 09:14:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:14:28,343] {spark_submit.py:495} INFO - 23/01/04 09:14:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:14:43,343] {spark_submit.py:495} INFO - 23/01/04 09:14:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2023-01-04 09:14:58,343] {spark_submit.py:495} INFO - 23/01/04 09:14:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
